# Weekly Report 


## M2KR 데이터셋 분석/정리

### M2KR에서 “코퍼스(corpus)”가 의미하는 것
- **Corpus = 검색 대상 후보 풀(knowledge/passages)**을 의미함.
- 쿼리(질문/이미지)마다 “정답 문서 1개”가 고정으로 붙는 구조가 아니라, **전체 코퍼스(큰 풀)에서 top-k를 검색해서** 컨텍스트를 구성하는 형태가 많음.
- 그래서 **쿼리 수보다 passage 수가 훨씬 클 수 있음**(예: OKVQA의 passages 114,810).


### InfoSeek: 원본 vs balanced(76K) 차이

핵심 관찰(전수 스캔):
- **스키마는 동일(9개 필드 동일)**: `answer, data_id, dataset_id, image_path, instruction, passage_id, passage_image_path, passage_text, question`
- **샘플 수**:
  - 원본: **676,441**
  - balanced: **76,265** (원본의 약 11.3%)
- **balanced는 분포를 단순 랜덤이 아니라 “균형화”한 subset으로 보임**
  - `passage_id` 후보 개수 버킷 분포(%) 변화:
    - 2–3개: **29.76% → 39.61%** (증가)
    - 11+개: **23.64% → 16.08%** (감소)
  - `passage_image_path`가 `NA`인 비율도 감소(대략 **4.31% → 2.17%**)
  - 질문 WH 타입 분포도 일부 변화(what/which 비중 상승)

## 이번에 선택한 데이터셋(서브셋)과 선택 이유

선택한 서브셋과 이유: 일단 query쪽에서도 text+image 조합과 document(passage)쪽에서도 text+image가 가능한 걸로 꾸림
- **`oven_train`**
  - 대규모 샘플(loaded 339,137)
- **`infoseek_balanced_76K_train`**
  - InfoSeek 원본은 매우 커서(676K) 시간/비용이 큼 → 일단 **76K balanced로 “적정 규모+균형” 학습 시간 측정**
- **`evqa_train`**
  - 큰 편(loaded 167,369)
- **`okvqa_mm_train` (derived subset)**, 해당 데이터셋 같은 경우는 document 쪽에서 image가 같이 있는 것이 절반정도 되는 것을 확인해서 image+text 같이 있는 것들로만 뽑아서 만듬.
  - “query/passage 모두 텍스트+이미지” 조건을 만족하는 샘플만으로 구성하여 **멀티모달 페어 학습 케이스** 측정

데이터셋 재구성 방식:
- **okvqa_mm_train 전체(8,253줄) 전수 검사**에서 다음 필드 결측 0:
  - query: `question`(텍스트), `image_path`(이미지)
  - passage: `passage_text`(텍스트), `passage_image_path`(이미지), `passage_id`
- `passage_id/passage_text/passage_image_path`는 전부 list 형태로 들어있고,
  - 학습 시 `datasets.py` 로더가 **그 list에서 1개를 랜덤 샘플링**하여 (query, passage) pair를 구성함.

의미:
- “passage 1개를 고정”하진 않지만, **매 step에서 선택되는 positive passage는 텍스트+이미지 둘 다 존재**하도록 구성된 subset임.

## 실험 환경/설정


### 모델(인코더)
- Text encoder: `wkcn/TinyCLIP-ViT-61M-32-Text-29M-LAION400M`, 일단 Tiny CLIP이 제공하는 모델 중에 가장 큰 것을 사용함, 시간 오래 안 걸리면 이걸로 해도 될 것 같아서임
- Vision encoder: `wkcn/TinyCLIP-ViT-61M-32-Text-29M-LAION400M`
- Layer strategy: 3, 7, 11 layer로 고정정

### 학습/런타임 설정(고정값)
`bench_logs_flex/nohup_run.log` 기준:
- `CUDA_VISIBLE_DEVICES=0`
- `NPROC_PER_NODE=1`
- `NUM_EPOCHS=1` (epoch-based)
- Warmup: `WARMUP_STEPS=20` (타이밍 제외), 데이터로딩 하는 시간들 제외
- Batch: `PER_DEVICE_TRAIN_BS=64`, `GRAD_ACCUM=1`
- LR: `2e-5`, Seed: `42`
- Dataloader: `NUM_WORKERS=12`, `PIN_MEMORY=False`
- Precision: `BF16=True`, `TF32=True`


## 실험 결과 (train만 1 epoch만 돌려봤을 때)

측정 방식:
- 각 서브셋을 **순차로 1 epoch 학습**

### 결과 테이블

| subset | loaded samples | steps | 1 epoch wall-clock(1epoch 돌았을 때 걸린 시간간) | max RSS | exit |
|---|---:|---:|---:|---:|---:|
| `oven_train` | 339,137 | 5,300 | **8:01.39** | ~3.15 GiB | 0 |
| `infoseek_bal76k_train` | 76,265 | 1,192 | **2:06.03** | ~2.59 GiB | 0 |
| `evqa_train` | 167,369 | 2,616 | **4:16.15** | ~2.35 GiB | 0 |
| `okvqa_mm_train` | 8,253 | 129 | **0:35.92** | ~1.88 GiB | 0 |

### 1 epoch 학습 성능(Train 로그 기반)
주의:
- 이번 실험은 `evaluation_strategy=no`라서 **검증셋 성능(eval)은 없음**. 아래는 **train 중에 로깅된 지표**임.

| subset | train_loss (epoch=1) | train r@1 (epoch=1) | 비고 |
|---|---:|---:|---|
| `oven_train` | 1.5284 | 0.8538 | r@1은 “최종 summary”가 아니라 **마지막 train log(epoch=1.0)** 에서 확인 |
| `infoseek_bal76k_train` | 1.7455 | 0.8151 | 최종 summary에 r@1 포함 |
| `evqa_train` | 1.6716 | 0.8046 | 최종 summary에 r@1 포함 |
| `okvqa_mm_train` | 3.6432 | 0.4884 | 최종 summary에 r@1 포함 |



### 전체 합(순차 실행)
- 총 소요: **약 14분 59초** (8:01 + 2:06 + 4:16 + 0:36)

### 다음 진행해야 할 것
- 일단 고른 데이터셋들 중 OKVQA도 balanced 된 것을 사용해서 시간이 이정도 나오면 전체 데이터셋으로 진행해도 될 것 같음
- val, test셋도 걸리는 시간 1 epoch 기준으로 측정
- MOBILE CLIP으로도 측정






