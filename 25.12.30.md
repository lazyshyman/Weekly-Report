# TinyCLIP 실험 결과 정리

## 실험 개요

TinyCLIP 모델 이용, ReT-2 아키텍처로 M2KR 데이터셋 4개(OVEN, EVQA, InfoSeek, OKVQA-MM)에 대한 멀티모달 검색 성능을 평가

### 모델 정보
- **Vision Encoder**: `wkcn/TinyCLIP-ViT-61M-32-Text-29M-LAION400M`
- **Text Encoder**: `wkcn/TinyCLIP-ViT-61M-32-Text-29M-LAION400M`
- **Layer Strategy**: `auto`
  - **Vision**: (3, 7, 11) - 12개 레이어
  - **Text**: (2, 5, 8) - 9개 레이어 

### 데이터셋 구성 (Full Data - 100% 사용)

| 데이터셋 | 학습 샘플 수 | 비율 |
|---------|------------|------|
| **oven** | 339,137 | 28.5% |
| **evqa** | 167,369 | 14.0% |
| **infoseek** | 676,441 | 56.8% |
| **okvqa_mm** | 8,253 | 0.7% |
| **Total** | **1,191,200** | **100%** |

검증 데이터: 각 데이터셋에서 5,000 샘플 사용해서 best model 선정

### 데이터셋 분석 및 목적

#### 1. OVEN (Object Video Event Network)
- **목적**: 객체, 비디오, 이벤트 간의 멀티모달 검색
- **특성**: 
  - 상대적으로 단순한 retrieval task임
  - Baseline 성능이 높음 (R@5 73.8%) → task 자체가 덜 복잡

**데이터 구조 예시:**
```json
{
  "data_id": "oven_entity_val_00000000",
  "dataset_id": "oven_validation_m2kr",
  "question": "what is this park called?",
  "instruction": "Using the provided image, obtain documents that address the subsequent question: ",
  "image_path": "oven_eval/image_downloads/gldv2/b/1/0/b100609c678d8ab8.jpg",
  "passage_text": "Nationals Park is a baseball stadium along the Anacostia River...",
  "passage_id": "Q517545",
  "passage_image_path": "infoseek/wikipedia_images_full/Q517/Q517545.jpg",
  "answer": "Nationals Park"
}
```

**특징:**
- Query: 이미지 + 질문 (텍스트)
- Passage: 텍스트 + 이미지 (모두 존재)
- Answer: 단일 문자열
- 질문이 이미지의 객체/장소를 식별하는 단순한 형태

#### 2. EVQA (Embodied Video Question Answering)
- **목적**: 비디오 기반 embodied AI 질문에 대한 답변 검색
- **특성**:
  - 비디오의 시간적/공간적 맥락 이해 필요
  - Embodied AI 시나리오에서의 공간적 추론 요구
  - 질문이 비디오 프레임 간의 관계와 물리적 움직임을 이해해야 함

**데이터 구조 예시:**
```json
{
  "data_id": "EVQA_0",
  "dataset_id": "evqa_val_m2kr",
  "question": "How big can this plant become?",
  "instruction": "With the provided image, gather documents that offer a solution to the question: ",
  "image_path": "iNaturalist_2021/val/09426_Plantae_Tracheophyta_Magnoliopsida_Rosales_Rosaceae_Prunus_laurocerasus/...",
  "passage_text": "Prunus laurocerasus is an evergreen shrub or small to medium-sized tree, growing to 5 to 15 metres (16 to 49 ft) tall...",
  "passage_id": "WikiWeb_Prunus laurocerasus_1",
  "passage_image_path": "AToMiC-Images-v0.2/images/3373/c94a63b5-1133-39a9-bd76-2a04377b7530.jpg",
  "answer": ["5 to 15 metres", "5 to 15 metres ", "5 to 15 metres"]
}
```

**특징:**
- Query: 이미지 + 질문 (텍스트) - 비디오 프레임을 이미지로 표현
- Passage: 텍스트 + 이미지 (모두 존재)
- Answer: 리스트 형태 (여러 정답 형식 허용)
- 질문이 생물/객체의 속성이나 행동에 대한 추론 필요

#### 3. InfoSeek (Knowledge-based Visual QA)
- **목적**: 외부 지식이 필요한 시각적 질문 답변
- **특성**:
  - 지식 기반 추론이 필요한 복잡한 VQA task
  - 질문에 답하기 위해 일반 상식이나 도메인 지식 필요
  - 이미지와 텍스트를 결합한 지식 추론

**데이터 구조 예시:**
```json
{
  "data_id": "infoseek_train_00000000",
  "dataset_id": "infoseek_train",
  "question": "What is the objective of this object?",
  "instruction": "Obtain documents that correspond to the inquiry alongside the provided image: ",
  "image_path": "imagenet21k/imagenet21k_resized/imagenet21k_train/n03507963/n03507963_3717.JPEG",
  "passage_text": "Energy conversion is the process of changing energy from one form to another...",
  "passage_id": "Q178185",
  "passage_image_path": "infoseek/wikipedia_images_full/Q178/Q178185.jpg",
  "answer": ["energy conversion", "energy transformation", "conversion"]
}
```

**특징:**
- Query: 이미지 + 질문 (텍스트)
- Passage: 텍스트와 이미지가 `null`인 경우가 존재 (passage_id만 존재)
- Answer: 리스트 형태 (개념/지식 기반 답변)
- 질문이 객체의 목적, 기능, 개념 등 추상적인 지식 추론 필요
- Passage 텍스트가 없는 경우가 있어 지식 기반 추론에 더 의존

#### 4. OKVQA-MM (OK-VQA Multimodal)
- **목적**: 외부 지식이 필요한 멀티모달 질문 답변 (query와 passage 모두 텍스트+이미지)
- **특성**:
  - 외부 지식 + 다중 모달 이해가 모두 필요
  - Query와 passage 모두 텍스트와 이미지를 포함하는 완전한 멀티모달 페어
  - 학습 데이터: 8,253 샘플 (0.7%) 

**데이터 구조 예시:**
```json
{
  "data_id": 3575865,
  "dataset_id": "okvqa_mm",
  "question": "What toy is this?",
  "instruction": "With the provided image, gather documents that offer a solution to the question: ",
  "image_path": "coco/val2014/COCO_val2014_000000357586.jpg",
  "passage_id": ["12533314", "17255922"],
  "passage_text": [
    "title: \"Teddy Bear, Duke & Psycho\" content: Teddy Bear, Duke & Psycho is the fifth studio album...",
    "title: \"Amr Salama\" content: Salama stated that \"[The Censorship Board] thought that this film provokes Christians...\""
  ],
  "passage_image_path": [
    "infoseek/wikipedia_images_full/Q769/Q7693980.jpg",
    "infoseek/wikipedia_images_full/Q110/Q11018819.jpg"
  ],
  "answer": ["teddy bear", "stuffed animal", "bear", ...]
}
```

**특징:**
- Query: 이미지 + 질문 (텍스트)
- Passage: **리스트 형태** - 여러 passage 후보 존재
  - 각 passage는 텍스트 + 이미지 포함 (완전한 멀티모달)
  - 학습 시 리스트에서 1개를 랜덤 샘플링하여 사용
- Answer: 리스트 형태 (여러 정답 허용)
- 질문이 외부 지식을 필요로 하는 복잡한 추론 필요
- Query와 passage 모두 이미지가 필수 → 완전한 멀티모달 매칭

---

## 실험 1: TinyCLIP Full (Epoch-based Training)

### 학습 설정

```bash
NUM_EPOCHS=20
PER_DEVICE_TRAIN_BS=64
GRAD_ACCUM=1
LR=5e-5
WARMUP_RATIO=0.1
LR_SCHEDULER_TYPE=cosine
BF16=True
TF32=True

EARLY_STOP_PATIENCE=3
EARLY_STOP_THRESHOLD=0.001
METRIC_FOR_BEST_MODEL=eval_macro_R@10
LOAD_BEST_MODEL_AT_END=True
```

### 학습 결과

- **총 학습 시간**: 약 38,824초 (약 10.8시간)
- **최종 Epoch**: 19.0 (20 epochs 완료)
- **Best Checkpoint**: `checkpoint-353647` (epoch 19.0)
- **Best Metric (eval_macro_R@10)**: 0.3343


### Test Set 성능 결과


**파라미터 수 비교:**

| 구성 요소 | TinyCLIP | CLIP ViT-B/32 | 차이 |
|---------|----------|---------------|------|
| **Vision Encoder** | 61M | 86M | -25M (-29.1%) |
| **Text Encoder** | 29M | 63M | -34M (-54.0%) |
| **전체 파라미터** | **90M** | **149M** | **-59M (-39.6%)** |

**세부 분석:**
- TinyCLIP은 CLIP ViT-B/32 대비 **39.6% 작음** (90M vs 149M, 약 60% 수준)
- Text Encoder가 특히 작음: 29M vs 63M (**54% 감소**)
- Vision Encoder도 작지만 상대적으로 덜 감소: 61M vs 86M (**29.1% 감소**)
- 전체적으로 CLIP ViT-B/32의 **약 1.66배 작은 모델** (149M / 90M ≈ 1.66)


### 전체 성능 요약

| 데이터셋 | Baseline R@5 | TinyCLIP R@5 | PR@5 | PR@5 vs R@5 차이 |
|---------|-------------|--------------|------|------------------|
| **OVEN** | 73.8% | **55.12%** | 53.44% | -1.68% (거의 동일) |
| **EVQA** | 36.1% | **8.27%** | 24.67% | +16.40% (큰 차이) |
| **InfoSeek** | 36.9% | **8.58%** | 31.01% | +22.43% (큰 차이) |
| **OKVQA-MM** | 12.0% | **1.59%** | 35.49% | +33.90% (매우 큰 차이) |

### 주요 발견사항

1. **OVEN은 상대적으로 양호한 성능**
   - 모델 크기 대비 합당한 성능 (-28.7%)
   - PR@5 ≈ R@5로 정확한 매칭이 잘 이루어짐
   - 상대적으로 단순한 retrieval task로 보임

2. **나머지 데이터셋들의 공통 패턴: PR@5 >> R@5**
   - 관련 문서는 찾지만 정확한 매칭 실패
   - 모델의 표현력/차별화 능력 부족 시사
   - 작은 데이터셋일수록 더 큰 차이 (OKVQA-MM: +33.90%)

3. **복잡한 작업일수록 성능 하락 크기 증가**
   - EVQA (embodied, 시간적 맥락): -80.3%
   - InfoSeek (복잡한 정보 검색): -77.6%
   - OKVQA-MM (외부 지식 필요): -89.0%


4. **차별화 능력 부족**
   - PR@5 >> R@5 패턴: 관련 후보는 찾지만 정확한 정답 매칭 실패
   - EVQA: PR@5 (24.67%) vs R@5 (8.27%) → +16.40% 차이
   - InfoSeek: PR@5 (31.01%) vs R@5 (8.58%) → +22.43% 차이
   - OKVQA-MM: PR@5 (35.49%) vs R@5 (1.59%) → +33.90% 차이
   - 유사한 후보들을 구분하지 못함 → fine-grained discrimination 어려움


